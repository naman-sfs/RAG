{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build a PDF QA bot with open source models. We show how to use commercially available opensource models to query your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installations\n",
    "# !pip install langchain==0.0.189\n",
    "# !pip install chromadb==0.3.25\n",
    "# !pip install pdfplumber==0.9.0\n",
    "# !pip install tiktoken==0.4.0\n",
    "# !pip install lxml==4.9.2\n",
    "# !pip install torch==2.0.1\n",
    "# !pip install transformers==4.29.2\n",
    "# !pip install accelerate==0.19.0\n",
    "# !pip install sentence-transformers==2.2.2\n",
    "# !pip install einops==0.6.1\n",
    "# !pip install xformers==0.0.20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Keys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from hyperplane.utils import is_jhub\n",
    "# import json\n",
    "# import os\n",
    "# try:\n",
    "    \n",
    "#     if is_jhub(): ##If using jupyter hub and conda base environment\n",
    "#         openaiKeyFile = '/root/.secret/openai_key.json'\n",
    "#     else:\n",
    "#         ## Storing as a secret on k8s cluster\n",
    "#         openaiKeyFile = '/etc/hyperplane/secrets/openai_key.json'\n",
    "#     with open(openaiKeyFile) as f:\n",
    "#         os.environ[\"OPENAI_API_KEY\"] = json.load(f)['openai_key']\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Loading LLM embedding models and generative models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Embedding Models\n",
    "* Opensource models used in the codebase are\n",
    "    * [INSTRUCTOR XL](https://huggingface.co/hkunlp/instructor-xl): Instructor xl is an instruction-finetuned text embedding model, that can generate embeddings tailored for any task instuction. The instruction for embedding text snippets is \"Represent the document for retrieval:\" and instruction for embedding user question is  \"Represent the question for retrieving supporting documents:\"\n",
    "    * [SBERT](https://huggingface.co/sentence-transformers/all-mpnet-base-v2): SBERT maps sentences and paragraphs to a vectore using BERT like model. It's a good starter when you're prototyping your application\n",
    "* Hugging faces [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) compares embedding models on  different tasks, Instructor XL ranks very high in it, even better than [OpenAI's ADA](https://platform.openai.com/docs/guides/embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_OPENAI_ADA = \"text-embedding-ada-002\"\n",
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Generation Models\n",
    "* Opensource models used in the codebase are\n",
    "    * [FlanT5 Models](https://huggingface.co/docs/transformers/model_doc/flan-t5): FlanT5 is text2text generator that is finetuned on a number of tasks like summarisation, question answering. It uses the encode-decoder architecture of transformers. The model is Apache 2.0 licensed. It can used commercially.\n",
    "    * [FastChatT5 3b Model](https://huggingface.co/sentence-transformers/all-mpnet-base-v2): It's a FlanT5 based chat model trained by finetuning FlanT5 on user chats from ChatGpt. The model is Apache 2.0 licensed.\n",
    "    * [Falcon7b Model](https://huggingface.co/tiiuae/falcon-7b): Falcon7b is a smaller version of Falcon which is text generator model (decoder only model). Falcon-40B is currently the best open source model on the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). They are trained with high quality training data\n",
    "\n",
    "There are other open-source models (MPT-7B, StableLM, RedPajama) in the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) you can consider for your tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LLM_OPENAI_GPT35 = \"gpt-3.5-turbo\"\n",
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Falcon, FlanT5 LARGE,XL,XXL, Fastchat model requires GPU to run. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go ahead and first setup SBERT for embedding model and  flant5-base for generation model. Their inference latency is decent on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Python\\anaconda3\\envs\\bot_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be loaded and used for inference with the help of [hugging face pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) which abstract away alot of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "\n",
    "def create_falcon_instruct_small(load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Python\\anaconda3\\envs\\bot_env\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Python\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Ingesting the data into vector store (ChromaDB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"B1.pdf\"\n",
    "loader = PDFPlumberLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split documents and create text snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "texts = text_splitter.split_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = config[\"persist_directory\"]\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Retriving Snippets and Prompt Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrievalQA finds relevant snippets based on question embeddings, then construct a Prompt and query LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "# , chain_kwargs = {\"return_intermediate_steps\":True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a default prompt for flan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "    question_t5_template = \"\"\"\n",
    "    context: {context}\n",
    "    question: {question}\n",
    "    answer: \n",
    "    \"\"\"\n",
    "    QUESTION_T5_PROMPT = PromptTemplate(\n",
    "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Querying LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=100) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'how can i improve my relation with my child?',\n",
       " 'result': 'a healthy attachment style',\n",
       " 'source_documents': [Document(page_content='Introduction to Family and Parenting 21\\nfor an explanation or compromise of the rules because there is no need for\\nsuch communication; therefore, when there is no objection to the rules as a\\nmatter of cultural inheritance; there is no defiant or insubordinate behavior\\nas a consequence. The parenting technique and disciplinary methods, and\\nthe degree of control and warmth showed, must also be measured against\\nthe cultural norms and the context of parenting (Sorkhabi, 2005).\\nIntentional Parenting Styles\\nIn the last few decades, new parenting practices have broken out of the\\nBaumrindian styles discussed previously such as the positive approach to\\nparenting, mindful approach to parenting, and awareness integration\\napproach to parenting. Positive parenting is based on relational dynamics\\nbetween the child and parent. This type of parenting relies on nurturing,\\nguidance, recognition of the child’s achievements, and setting appropriate\\nboundaries (Juffer et al., 2008). An increased state of awareness to\\nenvironmental stimuli, heightened attention and being in the moment or\\npresent are some of the tools mindful parents use to raise their children’s\\nself-consciousness, self-efficacy, and focus (Ceder, 2017). The awareness\\nintegration approach to parenting is based on the parent being respectful to\\nboth the parent’s and the child’s inner processes and guiding the child\\ntoward intentionality in both learning and creating (Zeine, 2017).\\nPositive Approach to Parenting\\nThe positive parenting style is based on the principles of attachment theory.\\nIt focuses on the quality of the parent-child relationship and the positive\\nimpact of this reciprocal interaction. Attachment, an invisible emotional tie\\nbetween the parent and child, is a developmental milestone that takes place\\nbetween the ages of 9 and 24 months old. The precursor of attachment is\\nbonding, a physical relationship that starts prenatally and continues until\\nattachment takes place. A healthy attachment style requires the caregiver’s\\nsensitivity and responsivity, and the mutually reciprocal relationships that\\nappear because of all the interactions (Juffer et al., 2008).\\nPositive parenting helps parents to develop and sustain their capacities for\\ncaring, teaching, leading, communicating, and providing for the needs of a\\nchild consistently and unconditionally (Seay et al., 2014). Positive parenting\\npositions parents so they can provide a nurturing and empowering environment\\nfor their children and recognize and acknowledge their children’s\\naccomplishments. Parents who use the positive parenting style give clear', metadata={'source': 'B1.pdf', 'file_path': 'B1.pdf', 'page': 35, 'total_pages': 273, 'Producer': 'macOS Version 14.2.1 (Build 23C71) Quartz PDFContext', 'Author': '.', 'Creator': 'Acrobat PDFMaker 22 for Word', 'CreationDate': \"D:20240221043351Z00'00'\", 'ModDate': \"D:20240221043351Z00'00'\"}),\n",
       "  Document(page_content='22 Chapter One\\nguidelines, provide consistent, non-violent, and unconditional love, and\\ndemonstrate respect for their children’s developmental stages and unique\\nneeds within each stage. The objective of positive parenting is to teach\\ndiscipline in a way that builds a child’s self-esteem and supports a mutually\\nrespectful parent-child relationship without breaking the child’s spirit\\n(Godfrey, 2019). The overall picture of positive parenting is that of a warm,\\nthoughtful, and loving – but not permissive parent.\\nQualities of Children Raised within a Positive Parenting Style\\nParental use of a warm, loving, and supportive style, results in children\\ndeveloping a strong sense of prosocial behavior, the ability to appropriately\\nfunction in social settings, and the understanding of social conventions.\\nChildren raised within a family that follows a positive parenting style can\\nfocus on their assets, express their emotions in a healthy way, and\\nappropriately handle problems (Eisenberg et al., 2005). Children’s\\nemotional modulation and expression are recognized in the habits they\\nformed during their early childhood years from being raised in positive\\nparenting families. Generally speaking, there are many aspects of positive\\nparenting that nurture children’s self-esteem, creativity, belief in a positive\\nfuture, ability to get along with others, and sense of mastery over their\\nenvironment.\\nMindful Approach to Parenting\\nBased on the Buddhist mentality, a mindful parenting style uses a parent’s\\nconsciousness and awareness of their child’s behavior in context. This\\nmeans that the parent focuses their attention and the child’s conscious\\nattention on what is happening in the present, instead of allowing emotions\\nto influence decisions and dictate behaviors. Children are positively\\ninfluenced when parents model how to deal with stressful situations and\\nmake positive decisions for their behaviors in a consistent manner. A\\nmindful parent regulates their own emotions and demonstrates sound\\ndecision-making, thereby teaching their child a set of skills that is needed to\\nfunction in all settings. Being a role model for children by no means\\nguarantees that one is functioning as a perfect parent; rather, being a role\\nmodel fosters a parent’s ability to be mindful of taking the time to make\\nconscious decisions, and to act as opposed to being reactive (Ceder, 2017).\\nEven at times when parents forget to be mindful or are distracted, they can\\nconsciously bring themselves back to be present.', metadata={'source': 'B1.pdf', 'file_path': 'B1.pdf', 'page': 36, 'total_pages': 273, 'Producer': 'macOS Version 14.2.1 (Build 23C71) Quartz PDFContext', 'Author': '.', 'Creator': 'Acrobat PDFMaker 22 for Word', 'CreationDate': \"D:20240221043351Z00'00'\", 'ModDate': \"D:20240221043351Z00'00'\"}),\n",
       "  Document(page_content='Introduction to Family and Parenting 25\\nproactive parents, teach the child how to regulate their emotions by doing\\nthe following: (a) becoming physiologically aware of the emotion they are\\nexperiencing and naming it; (b) focusing on the emotion; (c) recognizing\\nthe thought process behind the emotion; (d) releasing the emotion by a\\nvariety of releasing skills; (e) supporting the child to come up with the\\ndesired outcome; and (f) formulating the action plan toward obtaining the\\ndesired outcome that led to a change of their state of being.\\nQualities of Children Raised within an Awareness Integration\\nParenting Style\\nStructured and rule-based parenting, combined with constructive dialogue\\nwherein parents discuss boundaries, leads to children’s optimal development,\\nhigher self-esteem, enhanced emotional regulation, and healthier parent-\\nchild relationships (Zeine, 2021). Parents who use AIT modeling, share\\nrules and guidelines with children by explaining the reasons and benefits of\\nupholding the rules, and the costs and consequences of not upholding them.\\nThese rules can be explained by reflecting the realities of a rule-based\\nsociety. Children raised with this parenting style, learn to associate between\\nemotions and thought processes. They demonstrate higher self-esteem and\\nare responsible and accountable for formulating their own identities (Zeine,\\n2021). They can regulate their feelings, are loving and compassionate, and\\nproactively co-create their agency in their ever-changing world.\\nThe Cumulative and Changing Roles of Parents During\\nChild-Rearing Years\\nParents must be aware of their changing roles while raising their children.\\nEach role contains unique functions and responsibilities that progress with\\nthe child’s developmental needs and requirements. An infant’s need for\\nparental attention certainly changes as the child reaches toddlerhood or later\\npreschool years. Parents need to shift their thought processes and focus on\\neach child to reflect these developmental shifts accordingly. Parents are\\nconstantly learning and tackling newfound parenting challenges while they\\nare also going through developmental changes (Myers, 2011). Examples of\\nthe many roles that a parent will adapt to are demonstrated in Table 1-4.\\nToday’s millennials as parents, in many cultures, are re-inventing the art of\\nparenting and creating new concepts on how to raise children. Concepts,\\nsuch as positive parenting, mindful parenting, and awareness integration\\nparenting, reflect the need for parents to approach parenting and raising', metadata={'source': 'B1.pdf', 'file_path': 'B1.pdf', 'page': 39, 'total_pages': 273, 'Producer': 'macOS Version 14.2.1 (Build 23C71) Quartz PDFContext', 'Author': '.', 'Creator': 'Acrobat PDFMaker 22 for Word', 'CreationDate': \"D:20240221043351Z00'00'\", 'ModDate': \"D:20240221043351Z00'00'\"}),\n",
       "  Document(page_content='Developmental Theories and Educational Methods 39\\nExample. Children who receive love and care from their parents develop\\nnormal trust. They prefer to be with their parents or caregivers. Children\\nwho are abandoned or orphaned at a young age often develop a mistrust of\\nany adult figures. This lack of trust means a child does not develop a healthy\\nattachment to a parent or caregiver. Likewise, a child who has a “non-\\npresent” parent or parents often develops mistrust, which results in their\\ninability to attach to the parents.\\nAutonomy vs. Shame and Doubt Stage (1–3 years)\\nEgo Strength: Will, “I am what I will”\\n\\uf0b7 Children need to be self-reliant and independent.\\n\\uf0b7 Children want to do things for themselves.\\n\\uf0b7 Children are impulsive and want to do more than they can do.\\n\\uf0b7 Children experiencing shame and doubt can lead to feelings of\\nworthlessness and incompetence.\\n\\uf0b7 Children develop new motor skills and mental abilities.\\n\\uf0b7 Children will say “no” to everything, this “no” is to establish\\nautonomy at that moment and is not necessarily an accurate response.\\n\\uf0b7 Children question how much control they have over the environment.\\n\\uf0b7 Children act on the environments.\\n\\uf0b7 Children develop feelings of self-worth if they are successful in\\naccomplishing tasks.\\n\\uf0b7 Children vacillate between dependence and independence.\\n\\uf0b7 A child’s environment revolves around becoming “potty trained.”\\n\\uf0b7 Children tend to like things they can push and pull.\\n\\uf0b7 Children tend to like to take things apart and put them back together\\nagain.\\n\\uf0b7 Children learn to focus and become more engrossed in one activity.\\n\\uf0b7 Children seek order.\\n\\uf0b7 Children like things/toys that produce sound.\\n\\uf0b7 Children do not understand the concept of sharing.\\n\\uf0b7 Adults should give toys and other things without conditions attached.\\n\\uf0b7 The child needs a sense of “mineness” to reinforce the concept of\\nownership.\\n\\uf0b7 The child needs a security blanket, for example, a blanket, bottle,\\nstuffed animal, etc., that helps to establish security amid a changing\\nworld.\\n\\uf0b7 A child’s holding on and letting go are evident from holding water\\nin their mouth to calling everything in their environment “mine.”', metadata={'source': 'B1.pdf', 'file_path': 'B1.pdf', 'page': 53, 'total_pages': 273, 'Producer': 'macOS Version 14.2.1 (Build 23C71) Quartz PDFContext', 'Author': '.', 'Creator': 'Acrobat PDFMaker 22 for Word', 'CreationDate': \"D:20240221043351Z00'00'\", 'ModDate': \"D:20240221043351Z00'00'\"})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how can i improve my relation with my child?\"\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.return_source_documents = True\n",
    "qa({\"query\":question,})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pack all the functionalities into a class. We have also added Other opensource LLM Models into the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PdfQA:\n",
    "    def __init__(self,config:dict = {}):\n",
    "        self.config = config\n",
    "        self.embedding = None\n",
    "        self.vectordb = None\n",
    "        self.llm = None\n",
    "        self.qa = None\n",
    "        self.retriever = None\n",
    "\n",
    "    # The following class methods are useful to create global GPU model instances\n",
    "    # This way we don't need to reload models in an interactive app,\n",
    "    # and the same model instance can be used across multiple user sessions\n",
    "    @classmethod\n",
    "    def create_instructor_xl(cls):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceInstructEmbeddings(model_name=EMB_INSTRUCTOR_XL, model_kwargs={\"device\": device})\n",
    "    \n",
    "    @classmethod\n",
    "    def create_sbert_mpnet(cls):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "    \n",
    "    @classmethod\n",
    "    def create_flan_t5_xxl(cls, load_in_8bit=False):\n",
    "        # Local flan-t5-xxl with 8-bit quantization for inference\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=\"google/flan-t5-xxl\",\n",
    "            max_new_tokens=200,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_xl(cls, load_in_8bit=False):\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=\"google/flan-t5-xl\",\n",
    "            max_new_tokens=200,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_flan_t5_small(cls, load_in_8bit=False):\n",
    "        # Local flan-t5-small for inference\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-small\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_base(cls, load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_large(cls, load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_fastchat_t5_xl(cls, load_in_8bit=False):\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model = \"lmsys/fastchat-t5-3b-v1.0\",\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_falcon_instruct_small(cls, load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "    \n",
    "    def init_embeddings(self) -> None:\n",
    "        # OpenAI ada embeddings API\n",
    "        if self.config[\"embedding\"] == EMB_OPENAI_ADA:\n",
    "            self.embedding = OpenAIEmbeddings()\n",
    "        elif self.config[\"embedding\"] == EMB_INSTRUCTOR_XL:\n",
    "            # Local INSTRUCTOR-XL embeddings\n",
    "            if self.embedding is None:\n",
    "                self.embedding = PdfQA.create_instructor_xl()\n",
    "        elif self.config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "            ## this is for SBERT\n",
    "            if self.embedding is None:\n",
    "                self.embedding = PdfQA.create_sbert_mpnet()\n",
    "        else:\n",
    "            self.embedding = None ## DuckDb uses sbert embeddings\n",
    "            # raise ValueError(\"Invalid config\")\n",
    "\n",
    "    def init_models(self) -> None:\n",
    "        \"\"\" Initialize LLM models based on config \"\"\"\n",
    "        load_in_8bit = self.config.get(\"load_in_8bit\",False)\n",
    "        # OpenAI GPT 3.5 API\n",
    "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
    "            # OpenAI GPT 3.5 API\n",
    "            pass\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_SMALL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_small(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_base(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_large(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_XL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_xl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_XXL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_xxl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_fastchat_t5_xl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FALCON_SMALL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_falcon_instruct_small(load_in_8bit=load_in_8bit)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid config\")        \n",
    "    def vector_db_pdf(self) -> None:\n",
    "        \"\"\"\n",
    "        creates vector db for the embeddings and persists them or loads a vector db from the persist directory\n",
    "        \"\"\"\n",
    "        pdf_path = self.config.get(\"pdf_path\",None)\n",
    "        persist_directory = self.config.get(\"persist_directory\",None)\n",
    "        if persist_directory and os.path.exists(persist_directory):\n",
    "            ## Load from the persist db\n",
    "            self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
    "        elif pdf_path and os.path.exists(pdf_path):\n",
    "            ## 1. Extract the documents\n",
    "            loader = PDFPlumberLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            ## 2. Split the texts\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            # text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "            text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)  # This the encoding for text-embedding-ada-002\n",
    "            texts = text_splitter.split_documents(texts)\n",
    "\n",
    "            ## 3. Create Embeddings and add to chroma store\n",
    "            ##TODO: Validate if self.embedding is not None\n",
    "            self.vectordb = Chroma.from_documents(documents=texts, embedding=self.embedding, persist_directory=persist_directory)\n",
    "        else:\n",
    "            raise ValueError(\"NO PDF found\")\n",
    "\n",
    "    def retreival_qa_chain(self):\n",
    "        \"\"\"\n",
    "        Creates retrieval qa chain using vectordb as retrivar and LLM to complete the prompt\n",
    "        \"\"\"\n",
    "        ##TODO: Use custom prompt\n",
    "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "        \n",
    "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
    "          # Use ChatGPT API\n",
    "          self.qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.), chain_type=\"stuff\",\\\n",
    "                                      retriever=self.vectordb.as_retriever(search_kwargs={\"k\":3}))\n",
    "        else:\n",
    "            hf_llm = HuggingFacePipeline(pipeline=self.llm,model_id=self.config[\"llm\"])\n",
    "\n",
    "            self.qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=self.retriever)\n",
    "            if self.config[\"llm\"] == LLM_FLAN_T5_SMALL or self.config[\"llm\"] == LLM_FLAN_T5_BASE or self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "                question_t5_template = \"\"\"\n",
    "                context: {context}\n",
    "                question: {question}\n",
    "                answer: \n",
    "                \"\"\"\n",
    "                QUESTION_T5_PROMPT = PromptTemplate(\n",
    "                    template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "                )\n",
    "                self.qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\n",
    "            self.qa.combine_documents_chain.verbose = True\n",
    "            self.qa.return_source_documents = True\n",
    "    def answer_query(self,question:str) ->str:\n",
    "        \"\"\"\n",
    "        Answer the question\n",
    "        \"\"\"\n",
    "\n",
    "        answer_dict = self.qa({\"query\":question,})\n",
    "        print(answer_dict)\n",
    "        answer = answer_dict[\"result\"]\n",
    "        if self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
    "            answer = self._clean_fastchat_t5_output(answer)\n",
    "        return answer\n",
    "    def _clean_fastchat_t5_output(self, answer: str) -> str:\n",
    "        # Remove <pad> tags, double spaces, trailing newline\n",
    "        answer = re.sub(r\"<pad>\\s+\", \"\", answer)\n",
    "        answer = re.sub(r\"  \", \" \", answer)\n",
    "        answer = re.sub(r\"\\n$\", \"\", answer)\n",
    "        return answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the PDFQA class is set up, you can initialize and run it as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for PDFQA\n",
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          \"pdf_path\":\"wiki_data_short.pdf\"\n",
    "          }\n",
    "\n",
    "# Initialize PDFQA\n",
    "pdfqa = PdfQA(config=config)\n",
    "pdfqa.init_embeddings()\n",
    "pdfqa.init_models()\n",
    "\n",
    "# Create Vector DB \n",
    "pdfqa.vector_db_pdf()\n",
    "\n",
    "# Set up Retrieval QA Chain\n",
    "pdfqa.retreival_qa_chain()\n",
    "\n",
    "# Query the model\n",
    "question = \"what the reason for financial crisis?\"\n",
    "pdfqa.answer_query(question)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
